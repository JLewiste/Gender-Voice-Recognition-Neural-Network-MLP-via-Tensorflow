{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv(\"/Users/firdause/Downloads/Gender-Voice-Recognition-Neural-Network-MLP-via-Tensorflow-master/Dataset/voice.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...    centroid   meanfun    minfun  \\\n",
       "0   274.402906  0.893369  0.491918  ...    0.059781  0.084279  0.015702   \n",
       "1   634.613855  0.892193  0.513724  ...    0.066009  0.107937  0.015826   \n",
       "2  1024.927705  0.846389  0.478905  ...    0.077316  0.098706  0.015656   \n",
       "3     4.177296  0.963322  0.727232  ...    0.151228  0.088965  0.017798   \n",
       "4     4.333713  0.971955  0.783568  ...    0.135120  0.106398  0.016931   \n",
       "\n",
       "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n",
       "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n",
       "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n",
       "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n",
       "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select our target class\n",
    "label = data.pop(\"label\")\n",
    "\n",
    "# converts features from dataframe to np array\n",
    "features = data.values\n",
    "\n",
    "# convert train labels to one hots\n",
    "one_hot_labels = pd.get_dummies(label)\n",
    "\n",
    "# make np array\n",
    "np_one_hot_labels = one_hot_labels.values\n",
    "\n",
    "# split dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, np_one_hot_labels, test_size=0.3)\n",
    "\n",
    "# convert the training and test set into numpy array\n",
    "# Tensorflow requires data in the form of numpy array\n",
    "# numpy array training set\n",
    "np_X_train = np.array(X_train,dtype='float32')\n",
    "np_y_train = np.array(y_train,dtype='float32')\n",
    "\n",
    "# numpy array testing set\n",
    "np_X_test = np.array(X_test,dtype='float32')\n",
    "np_y_test = np.array(y_test,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ..., \n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(np_one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:  (2217, 20)\n",
      "Testing set shape:  (951, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set shape: \", np_X_train.shape)\n",
    "print(\"Testing set shape: \", np_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Neural Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parametersL tune accordingly\n",
    "total_input = X_train.shape[1]\n",
    "total_output = y_train.shape[1]\n",
    "\n",
    "# static learning rate\n",
    "# lr = 0.001\n",
    "\n",
    "# decaying learning rate\n",
    "lr = tf.train.exponential_decay(learning_rate=0.001, # Base learning rate.\n",
    "                                global_step=100,    # current index into the dataset.\n",
    "                                decay_steps=10000,  # decay step.\n",
    "                                decay_rate=0.96,    # decay rate.\n",
    "                                staircase=True)     # If True decay the learning rate at discrete intervals\n",
    "total_epochs = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# place holder for inputs and outputs\n",
    "X = tf.placeholder(\"float\", [None, total_input])\n",
    "Y_ = tf.placeholder(\"float\", [None, total_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# weight and bias updates\n",
    "w1 = tf.Variable(tf.truncated_normal([20, 10], stddev=.5, name='w1'))\n",
    "b1 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([10, 10], stddev=.5, name='w2'))\n",
    "b2 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "w3 = tf.Variable(tf.truncated_normal([10, 10], stddev=.5, name='w3'))\n",
    "b3 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "w4 = tf.Variable(tf.truncated_normal([10, 2], stddev=.5, name='w4'))\n",
    "b4 = tf.Variable(tf.zeros([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hidden layers with RELU activation functions\n",
    "# optional: dropout layer\n",
    "layer_1 = tf.add(tf.matmul(X, w1), b1)\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "layer_1 = tf.nn.dropout(layer_1, 0.95)\n",
    "\n",
    "layer_2 = tf.add(tf.matmul(layer_1, w2), b2)\n",
    "layer_2 = tf.nn.relu(layer_2)\n",
    "layer_2 = tf.nn.dropout(layer_2, 0.95)\n",
    "\n",
    "layer_3 = tf.add(tf.matmul(layer_2, w3), b3)\n",
    "layer_3 = tf.nn.relu(layer_3)\n",
    "layer_3 = tf.nn.dropout(layer_3, 0.95)\n",
    "\n",
    "# final layer does not have activation function!\n",
    "output_layer = tf.add(tf.matmul(layer_3, w4), b4)\n",
    "\n",
    "Y = tf.nn.softmax(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost function\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(Y, Y_, name='cross_entropy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# back-propagation via Adam optimizer\n",
    "opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "# train step which minimizes the weight and bias variables\n",
    "train_step = opt.minimize(loss, var_list=[w1, b1, w2, b2, w3, b3, w4, b4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy metric\n",
    "tf_correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "tf_accuracy = tf.reduce_mean(tf.cast(tf_correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain mini batch\n",
    "def get_mini_batch(X, Y_):\n",
    "    rows=np.random.choice(X.shape[0], 100)\n",
    "    return X[rows], Y_[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 and Loss: 73.03720092773438\n",
      "Epoch: 100 and Loss: 68.36104583740234\n",
      "Epoch: 200 and Loss: 68.90569305419922\n",
      "Epoch: 300 and Loss: 61.53878402709961\n",
      "Epoch: 400 and Loss: 61.58378601074219\n",
      "Epoch: 500 and Loss: 64.72464752197266\n",
      "Epoch: 600 and Loss: 57.0030403137207\n",
      "Epoch: 700 and Loss: 59.469730377197266\n",
      "Epoch: 800 and Loss: 59.019432067871094\n",
      "Epoch: 900 and Loss: 60.33428955078125\n",
      "Epoch: 1000 and Loss: 56.53525924682617\n",
      "Epoch: 1100 and Loss: 52.867958068847656\n",
      "Epoch: 1200 and Loss: 57.017822265625\n",
      "Epoch: 1300 and Loss: 51.32432556152344\n",
      "Epoch: 1400 and Loss: 54.64397430419922\n",
      "Epoch: 1500 and Loss: 51.48332214355469\n",
      "Epoch: 1600 and Loss: 57.064369201660156\n",
      "Epoch: 1700 and Loss: 51.18265914916992\n",
      "Epoch: 1800 and Loss: 56.546165466308594\n",
      "Epoch: 1900 and Loss: 53.711021423339844\n",
      "Epoch: 2000 and Loss: 52.643218994140625\n",
      "Epoch: 2100 and Loss: 49.62469482421875\n",
      "Epoch: 2200 and Loss: 53.20357131958008\n",
      "Epoch: 2300 and Loss: 45.57492446899414\n",
      "Epoch: 2400 and Loss: 48.836769104003906\n",
      "Epoch: 2500 and Loss: 55.00992965698242\n",
      "Epoch: 2600 and Loss: 43.98066711425781\n",
      "Epoch: 2700 and Loss: 44.983856201171875\n",
      "Epoch: 2800 and Loss: 48.72835922241211\n",
      "Epoch: 2900 and Loss: 44.75227737426758\n",
      "Epoch: 3000 and Loss: 54.78884506225586\n",
      "Epoch: 3100 and Loss: 47.12298583984375\n",
      "Epoch: 3200 and Loss: 43.17019271850586\n",
      "Epoch: 3300 and Loss: 47.194393157958984\n",
      "Epoch: 3400 and Loss: 47.23001480102539\n",
      "Epoch: 3500 and Loss: 54.32109832763672\n",
      "Epoch: 3600 and Loss: 48.10547637939453\n",
      "Epoch: 3700 and Loss: 44.10329818725586\n",
      "Epoch: 3800 and Loss: 46.645851135253906\n",
      "Epoch: 3900 and Loss: 44.236934661865234\n",
      "Epoch: 4000 and Loss: 50.82881164550781\n",
      "Epoch: 4100 and Loss: 49.49196243286133\n",
      "Epoch: 4200 and Loss: 44.3292236328125\n",
      "Epoch: 4300 and Loss: 54.26995849609375\n",
      "Epoch: 4400 and Loss: 42.50839614868164\n",
      "Epoch: 4500 and Loss: 41.48553466796875\n",
      "Epoch: 4600 and Loss: 42.84648132324219\n",
      "Epoch: 4700 and Loss: 44.20812225341797\n",
      "Epoch: 4800 and Loss: 48.567386627197266\n",
      "Epoch: 4900 and Loss: 46.85795974731445\n",
      "Epoch: 5000 and Loss: 42.93968963623047\n",
      "Epoch: 5100 and Loss: 46.51911163330078\n",
      "Epoch: 5200 and Loss: 46.95755386352539\n",
      "Epoch: 5300 and Loss: 47.28127670288086\n",
      "Epoch: 5400 and Loss: 50.57384490966797\n",
      "Epoch: 5500 and Loss: 41.699256896972656\n",
      "Epoch: 5600 and Loss: 39.65234375\n",
      "Epoch: 5700 and Loss: 51.7845458984375\n",
      "Epoch: 5800 and Loss: 48.90436553955078\n",
      "Epoch: 5900 and Loss: 50.40681838989258\n",
      "Epoch: 6000 and Loss: 48.88529968261719\n",
      "Epoch: 6100 and Loss: 47.817527770996094\n",
      "Epoch: 6200 and Loss: 39.72335433959961\n",
      "Epoch: 6300 and Loss: 45.64720916748047\n",
      "Epoch: 6400 and Loss: 39.74476623535156\n",
      "Epoch: 6500 and Loss: 45.43718719482422\n",
      "Epoch: 6600 and Loss: 42.58411407470703\n",
      "Epoch: 6700 and Loss: 44.62969207763672\n",
      "Epoch: 6800 and Loss: 44.76434326171875\n",
      "Epoch: 6900 and Loss: 40.65370178222656\n",
      "Epoch: 7000 and Loss: 42.70050048828125\n",
      "Epoch: 7100 and Loss: 40.76009750366211\n",
      "Epoch: 7200 and Loss: 41.99317169189453\n",
      "Epoch: 7300 and Loss: 48.01091003417969\n",
      "Epoch: 7400 and Loss: 45.019142150878906\n",
      "Epoch: 7500 and Loss: 44.00334167480469\n",
      "Epoch: 7600 and Loss: 46.46826171875\n",
      "Epoch: 7700 and Loss: 42.51092529296875\n",
      "Epoch: 7800 and Loss: 39.70382308959961\n",
      "Epoch: 7900 and Loss: 39.90354919433594\n",
      "Epoch: 8000 and Loss: 39.984527587890625\n",
      "Epoch: 8100 and Loss: 40.44407653808594\n",
      "Epoch: 8200 and Loss: 44.3765983581543\n",
      "Epoch: 8300 and Loss: 45.37276077270508\n",
      "Epoch: 8400 and Loss: 40.88737869262695\n",
      "Epoch: 8500 and Loss: 46.27739715576172\n",
      "Epoch: 8600 and Loss: 43.045166015625\n",
      "Epoch: 8700 and Loss: 38.5735969543457\n",
      "Epoch: 8800 and Loss: 48.6627082824707\n",
      "Epoch: 8900 and Loss: 45.300655364990234\n",
      "Epoch: 9000 and Loss: 45.195220947265625\n",
      "Epoch: 9100 and Loss: 44.2574577331543\n",
      "Epoch: 9200 and Loss: 48.37192916870117\n",
      "Epoch: 9300 and Loss: 42.84239196777344\n",
      "Epoch: 9400 and Loss: 41.001747131347656\n",
      "Epoch: 9500 and Loss: 43.6962890625\n",
      "Epoch: 9600 and Loss: 44.604679107666016\n",
      "Epoch: 9700 and Loss: 45.445533752441406\n",
      "Epoch: 9800 and Loss: 44.29559326171875\n",
      "Epoch: 9900 and Loss: 38.434356689453125\n",
      "Epoch: 10000 and Loss: 41.62078857421875\n",
      "Epoch: 10100 and Loss: 43.25013732910156\n",
      "Epoch: 10200 and Loss: 43.9350471496582\n",
      "Epoch: 10300 and Loss: 39.376617431640625\n",
      "Epoch: 10400 and Loss: 42.41274642944336\n",
      "Epoch: 10500 and Loss: 43.1074104309082\n",
      "Epoch: 10600 and Loss: 42.4669189453125\n",
      "Epoch: 10700 and Loss: 42.09874725341797\n",
      "Epoch: 10800 and Loss: 43.722232818603516\n",
      "Epoch: 10900 and Loss: 43.67591857910156\n",
      "Epoch: 11000 and Loss: 40.243656158447266\n",
      "Epoch: 11100 and Loss: 39.8719482421875\n",
      "Epoch: 11200 and Loss: 38.65340805053711\n",
      "Epoch: 11300 and Loss: 42.73229217529297\n",
      "Epoch: 11400 and Loss: 40.700679779052734\n",
      "Epoch: 11500 and Loss: 37.64305877685547\n",
      "Epoch: 11600 and Loss: 40.93065643310547\n",
      "Epoch: 11700 and Loss: 40.08111572265625\n",
      "Epoch: 11800 and Loss: 39.937416076660156\n",
      "Epoch: 11900 and Loss: 39.759925842285156\n",
      "Epoch: 12000 and Loss: 49.259620666503906\n",
      "Epoch: 12100 and Loss: 34.72203826904297\n",
      "Epoch: 12200 and Loss: 39.002037048339844\n",
      "Epoch: 12300 and Loss: 42.21160125732422\n",
      "Epoch: 12400 and Loss: 39.31177520751953\n",
      "Epoch: 12500 and Loss: 38.76476287841797\n",
      "Epoch: 12600 and Loss: 41.67881774902344\n",
      "Epoch: 12700 and Loss: 40.15016174316406\n",
      "Epoch: 12800 and Loss: 39.976993560791016\n",
      "Epoch: 12900 and Loss: 46.86053466796875\n",
      "Epoch: 13000 and Loss: 38.660972595214844\n",
      "Epoch: 13100 and Loss: 39.379249572753906\n",
      "Epoch: 13200 and Loss: 39.677772521972656\n",
      "Epoch: 13300 and Loss: 44.25859832763672\n",
      "Epoch: 13400 and Loss: 40.96942138671875\n",
      "Epoch: 13500 and Loss: 38.128684997558594\n",
      "Epoch: 13600 and Loss: 36.87816619873047\n",
      "Epoch: 13700 and Loss: 38.13426971435547\n",
      "Epoch: 13800 and Loss: 47.104061126708984\n",
      "Epoch: 13900 and Loss: 36.11600875854492\n",
      "Epoch: 14000 and Loss: 39.321590423583984\n",
      "Epoch: 14100 and Loss: 36.632720947265625\n",
      "Epoch: 14200 and Loss: 38.941261291503906\n",
      "Epoch: 14300 and Loss: 35.560123443603516\n",
      "Epoch: 14400 and Loss: 39.609344482421875\n",
      "Epoch: 14500 and Loss: 40.09107208251953\n",
      "Epoch: 14600 and Loss: 37.670719146728516\n",
      "Epoch: 14700 and Loss: 39.789432525634766\n",
      "Epoch: 14800 and Loss: 38.609710693359375\n",
      "Epoch: 14900 and Loss: 44.08526611328125\n",
      "Epoch: 15000 and Loss: 41.57224655151367\n",
      "Epoch: 15100 and Loss: 38.29352569580078\n",
      "Epoch: 15200 and Loss: 38.18009948730469\n",
      "Epoch: 15300 and Loss: 38.04888916015625\n",
      "Epoch: 15400 and Loss: 37.96720886230469\n",
      "Epoch: 15500 and Loss: 41.58488082885742\n",
      "Epoch: 15600 and Loss: 39.755348205566406\n",
      "Epoch: 15700 and Loss: 37.500511169433594\n",
      "Epoch: 15800 and Loss: 38.63896942138672\n",
      "Epoch: 15900 and Loss: 39.503662109375\n",
      "Epoch: 16000 and Loss: 35.60498809814453\n",
      "Epoch: 16100 and Loss: 40.56881332397461\n",
      "Epoch: 16200 and Loss: 40.148643493652344\n",
      "Epoch: 16300 and Loss: 44.54197692871094\n",
      "Epoch: 16400 and Loss: 39.545745849609375\n",
      "Epoch: 16500 and Loss: 35.74260711669922\n",
      "Epoch: 16600 and Loss: 39.58183670043945\n",
      "Epoch: 16700 and Loss: 35.54109573364258\n",
      "Epoch: 16800 and Loss: 38.67597579956055\n",
      "Epoch: 16900 and Loss: 39.390846252441406\n",
      "Epoch: 17000 and Loss: 35.861778259277344\n",
      "Epoch: 17100 and Loss: 37.7230224609375\n",
      "Epoch: 17200 and Loss: 42.015499114990234\n",
      "Epoch: 17300 and Loss: 44.58201599121094\n",
      "Epoch: 17400 and Loss: 37.082645416259766\n",
      "Epoch: 17500 and Loss: 37.62596130371094\n",
      "Epoch: 17600 and Loss: 41.34713363647461\n",
      "Epoch: 17700 and Loss: 37.08568572998047\n",
      "Epoch: 17800 and Loss: 35.09526062011719\n",
      "Epoch: 17900 and Loss: 33.80138397216797\n",
      "Epoch: 18000 and Loss: 38.02146530151367\n",
      "Epoch: 18100 and Loss: 38.821537017822266\n",
      "Epoch: 18200 and Loss: 34.94983673095703\n",
      "Epoch: 18300 and Loss: 35.9495849609375\n",
      "Epoch: 18400 and Loss: 43.135093688964844\n",
      "Epoch: 18500 and Loss: 44.90771484375\n",
      "Epoch: 18600 and Loss: 39.819664001464844\n",
      "Epoch: 18700 and Loss: 34.95763397216797\n",
      "Epoch: 18800 and Loss: 37.43866729736328\n",
      "Epoch: 18900 and Loss: 34.135009765625\n",
      "Epoch: 19000 and Loss: 35.02558135986328\n",
      "Epoch: 19100 and Loss: 38.11075973510742\n",
      "Epoch: 19200 and Loss: 37.895198822021484\n",
      "Epoch: 19300 and Loss: 36.83753967285156\n",
      "Epoch: 19400 and Loss: 41.468379974365234\n",
      "Epoch: 19500 and Loss: 34.06787109375\n",
      "Epoch: 19600 and Loss: 35.442413330078125\n",
      "Epoch: 19700 and Loss: 37.11933898925781\n",
      "Epoch: 19800 and Loss: 37.512855529785156\n",
      "Epoch: 19900 and Loss: 39.64282989501953\n",
      "Epoch: 20000 and Loss: 40.99022674560547\n",
      "Epoch: 20100 and Loss: 39.79530334472656\n",
      "Epoch: 20200 and Loss: 39.48243713378906\n",
      "Epoch: 20300 and Loss: 34.02435302734375\n",
      "Epoch: 20400 and Loss: 33.861083984375\n",
      "Epoch: 20500 and Loss: 44.116493225097656\n",
      "Epoch: 20600 and Loss: 34.58829879760742\n",
      "Epoch: 20700 and Loss: 38.836708068847656\n",
      "Epoch: 20800 and Loss: 36.55097961425781\n",
      "Epoch: 20900 and Loss: 35.75218200683594\n",
      "Epoch: 21000 and Loss: 40.02012252807617\n",
      "Epoch: 21100 and Loss: 39.28148651123047\n",
      "Epoch: 21200 and Loss: 35.68055725097656\n",
      "Epoch: 21300 and Loss: 35.87208557128906\n",
      "Epoch: 21400 and Loss: 38.73948287963867\n",
      "Epoch: 21500 and Loss: 36.247432708740234\n",
      "Epoch: 21600 and Loss: 35.524330139160156\n",
      "Epoch: 21700 and Loss: 37.542388916015625\n",
      "Epoch: 21800 and Loss: 33.58671569824219\n",
      "Epoch: 21900 and Loss: 35.531097412109375\n",
      "Epoch: 22000 and Loss: 41.997528076171875\n",
      "Epoch: 22100 and Loss: 34.21046829223633\n",
      "Epoch: 22200 and Loss: 34.92680358886719\n",
      "Epoch: 22300 and Loss: 44.7498779296875\n",
      "Epoch: 22400 and Loss: 39.596961975097656\n",
      "Epoch: 22500 and Loss: 37.332305908203125\n",
      "Epoch: 22600 and Loss: 35.67353820800781\n",
      "Epoch: 22700 and Loss: 34.58074188232422\n",
      "Epoch: 22800 and Loss: 37.18448257446289\n",
      "Epoch: 22900 and Loss: 38.31520080566406\n",
      "Epoch: 23000 and Loss: 38.65985870361328\n",
      "Epoch: 23100 and Loss: 36.68878936767578\n",
      "Epoch: 23200 and Loss: 38.132354736328125\n",
      "Epoch: 23300 and Loss: 44.21208190917969\n",
      "Epoch: 23400 and Loss: 36.34339904785156\n",
      "Epoch: 23500 and Loss: 34.50244140625\n",
      "Epoch: 23600 and Loss: 41.09734344482422\n",
      "Epoch: 23700 and Loss: 34.261573791503906\n",
      "Epoch: 23800 and Loss: 33.58883285522461\n",
      "Epoch: 23900 and Loss: 35.9305419921875\n",
      "Epoch: 24000 and Loss: 35.42945861816406\n",
      "Epoch: 24100 and Loss: 40.467323303222656\n",
      "Epoch: 24200 and Loss: 33.808013916015625\n",
      "Epoch: 24300 and Loss: 40.16107940673828\n",
      "Epoch: 24400 and Loss: 35.52101516723633\n",
      "Epoch: 24500 and Loss: 39.127418518066406\n",
      "Epoch: 24600 and Loss: 38.437679290771484\n",
      "Epoch: 24700 and Loss: 34.05316162109375\n",
      "Epoch: 24800 and Loss: 36.06037902832031\n",
      "Epoch: 24900 and Loss: 35.100135803222656\n",
      "\n",
      "Test accuracy: 0.9600420594215393\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:    \n",
    "    # init all variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(total_epochs):\n",
    "        # get mini batch\n",
    "        a, b = get_mini_batch(X_train, y_train)\n",
    "\n",
    "        # run train step, feeding arrays of 100 rows each time\n",
    "        _, cost = sess.run([train_step, loss], feed_dict={X: a, Y_: b})\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch: {0} and Loss: {1}\".format(i, cost))\n",
    "    \n",
    "    # benchmark neural network performance\n",
    "    result = sess.run(tf_accuracy, feed_dict={X: X_test, Y_: y_test})\n",
    "    print()\n",
    "    print(\"Test accuracy: {}\".format(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
