{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv(\"/Users/firdause/Downloads/Gender Voice Recognition/voice.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...    centroid   meanfun    minfun  \\\n",
       "0   274.402906  0.893369  0.491918  ...    0.059781  0.084279  0.015702   \n",
       "1   634.613855  0.892193  0.513724  ...    0.066009  0.107937  0.015826   \n",
       "2  1024.927705  0.846389  0.478905  ...    0.077316  0.098706  0.015656   \n",
       "3     4.177296  0.963322  0.727232  ...    0.151228  0.088965  0.017798   \n",
       "4     4.333713  0.971955  0.783568  ...    0.135120  0.106398  0.016931   \n",
       "\n",
       "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n",
       "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n",
       "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n",
       "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n",
       "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # create more data\n",
    "# for i in range(6):\n",
    "#     copy = voice\n",
    "#     copy['meanfreq']=copy['meanfreq']+random.gauss(.0001,.001) # add noice to mean freq var\n",
    "#     voice=voice.append(copy,ignore_index=True) # make voice df 2x as big\n",
    "#     print(\"shape of df after {0}th intertion of this loop is {1}\".format(i,voice.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select our target class\n",
    "label = data.pop(\"label\")\n",
    "\n",
    "# converts features from dataframe to np array\n",
    "features = data.values\n",
    "\n",
    "# convert train labels to one hots\n",
    "one_hot_labels = pd.get_dummies(label)\n",
    "\n",
    "# make np array\n",
    "np_one_hot_labels = one_hot_labels.values\n",
    "\n",
    "# split dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, np_one_hot_labels, test_size=0.3)\n",
    "\n",
    "# convert the training and test set into numpy array\n",
    "# Tensorflow requires data in the form of numpy array\n",
    "# numpy array training set\n",
    "np_X_train = np.array(X_train,dtype='float32')\n",
    "np_y_train = np.array(y_train,dtype='float32')\n",
    "\n",
    "# numpy array testing set\n",
    "np_X_test = np.array(X_test,dtype='float32')\n",
    "np_y_test = np.array(y_test,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ..., \n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(np_one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:  (2217, 20)\n",
      "Testing set shape:  (951, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set shape: \", np_X_train.shape)\n",
    "print(\"Testing set shape: \", np_X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Neural Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "total_input = X_train.shape[1]\n",
    "total_output = y_train.shape[1]\n",
    "\n",
    "learning_rate = 0.001\n",
    "total_epochs = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# place holder for inputs and outputs\n",
    "x = tf.placeholder(\"float\", [None, total_input])\n",
    "y = tf.placeholder(\"float\", [None, total_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weight and bias updates\n",
    "w1 = tf.Variable(tf.random_normal([20, 10], stddev=.5, name='w1'))\n",
    "b1 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([10, 10], stddev=.5, name='w2'))\n",
    "b2 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "w3 = tf.Variable(tf.random_normal([10, 10], stddev=.5, name='w3'))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "w4 = tf.Variable(tf.random_normal([10, 2], stddev=.5, name='w4'))\n",
    "b4 = tf.Variable(tf.random_normal([2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hidden layers and RELU activation functions\n",
    "layer_1 = tf.add(tf.matmul(x, w1), b1)\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "layer_2 = tf.add(tf.matmul(layer_1, w2), b2)\n",
    "layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "layer_3 = tf.add(tf.matmul(layer_2, w3), b3)\n",
    "layer_3 = tf.nn.relu(layer_3)\n",
    "\n",
    "# final layer does not have activation function!\n",
    "output_layer = tf.add(tf.matmul(layer_3, w4), b4)\n",
    "\n",
    "y_ = tf.nn.softmax(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost function\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(y_, y, name='cross_entropy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# back-propagation via Adam optimizer\n",
    "opt = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# train step which minimizes the weight and bias variables\n",
    "train_step = opt.minimize(loss, var_list=[w1, b1, w2, b2, w3, b3, w4, b4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy metric\n",
    "tf_correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "tf_accuracy = tf.reduce_mean(tf.cast(tf_correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain mini batch\n",
    "def get_mini_batch(x,y):\n",
    "    rows=np.random.choice(x.shape[0], 100)\n",
    "    return x[rows], y[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 and Loss: 83.37107849121094\n",
      "Epoch: 100 and Loss: 68.99403381347656\n",
      "Epoch: 200 and Loss: 61.23981475830078\n",
      "Epoch: 300 and Loss: 54.65131759643555\n",
      "Epoch: 400 and Loss: 60.84671401977539\n",
      "Epoch: 500 and Loss: 50.829612731933594\n",
      "Epoch: 600 and Loss: 50.65763854980469\n",
      "Epoch: 700 and Loss: 57.19831848144531\n",
      "Epoch: 800 and Loss: 50.58240509033203\n",
      "Epoch: 900 and Loss: 47.05940628051758\n",
      "Epoch: 1000 and Loss: 46.87858963012695\n",
      "Epoch: 1100 and Loss: 42.5968132019043\n",
      "Epoch: 1200 and Loss: 45.7943115234375\n",
      "Epoch: 1300 and Loss: 42.34330749511719\n",
      "Epoch: 1400 and Loss: 43.73741912841797\n",
      "Epoch: 1500 and Loss: 43.693450927734375\n",
      "Epoch: 1600 and Loss: 50.16815185546875\n",
      "Epoch: 1700 and Loss: 44.69630432128906\n",
      "Epoch: 1800 and Loss: 42.6500244140625\n",
      "Epoch: 1900 and Loss: 45.26355743408203\n",
      "Epoch: 2000 and Loss: 41.60751724243164\n",
      "Epoch: 2100 and Loss: 39.9048957824707\n",
      "Epoch: 2200 and Loss: 40.03375244140625\n",
      "Epoch: 2300 and Loss: 39.78605651855469\n",
      "Epoch: 2400 and Loss: 44.78984832763672\n",
      "Epoch: 2500 and Loss: 40.994136810302734\n",
      "Epoch: 2600 and Loss: 39.47783660888672\n",
      "Epoch: 2700 and Loss: 41.44960403442383\n",
      "Epoch: 2800 and Loss: 39.34939193725586\n",
      "Epoch: 2900 and Loss: 38.045616149902344\n",
      "Epoch: 3000 and Loss: 41.969444274902344\n",
      "Epoch: 3100 and Loss: 39.822731018066406\n",
      "Epoch: 3200 and Loss: 47.00831604003906\n",
      "Epoch: 3300 and Loss: 42.518367767333984\n",
      "Epoch: 3400 and Loss: 38.07891845703125\n",
      "Epoch: 3500 and Loss: 50.14374542236328\n",
      "Epoch: 3600 and Loss: 43.308815002441406\n",
      "Epoch: 3700 and Loss: 43.18632507324219\n",
      "Epoch: 3800 and Loss: 39.893592834472656\n",
      "Epoch: 3900 and Loss: 45.405643463134766\n",
      "Epoch: 4000 and Loss: 43.34671401977539\n",
      "Epoch: 4100 and Loss: 40.02716827392578\n",
      "Epoch: 4200 and Loss: 36.49885559082031\n",
      "Epoch: 4300 and Loss: 37.30205154418945\n",
      "Epoch: 4400 and Loss: 39.98988342285156\n",
      "Epoch: 4500 and Loss: 39.06622314453125\n",
      "Epoch: 4600 and Loss: 41.140296936035156\n",
      "Epoch: 4700 and Loss: 35.86627197265625\n",
      "Epoch: 4800 and Loss: 41.337562561035156\n",
      "Epoch: 4900 and Loss: 38.562660217285156\n",
      "Epoch: 5000 and Loss: 40.395469665527344\n",
      "Epoch: 5100 and Loss: 39.97959899902344\n",
      "Epoch: 5200 and Loss: 37.017189025878906\n",
      "Epoch: 5300 and Loss: 45.06163787841797\n",
      "Epoch: 5400 and Loss: 34.84022521972656\n",
      "Epoch: 5500 and Loss: 36.74662780761719\n",
      "Epoch: 5600 and Loss: 40.143394470214844\n",
      "Epoch: 5700 and Loss: 37.74512481689453\n",
      "Epoch: 5800 and Loss: 40.231632232666016\n",
      "Epoch: 5900 and Loss: 40.43547058105469\n",
      "Epoch: 6000 and Loss: 43.95851516723633\n",
      "Epoch: 6100 and Loss: 43.31934356689453\n",
      "Epoch: 6200 and Loss: 42.596046447753906\n",
      "Epoch: 6300 and Loss: 37.98328399658203\n",
      "Epoch: 6400 and Loss: 39.41243362426758\n",
      "Epoch: 6500 and Loss: 39.40882873535156\n",
      "Epoch: 6600 and Loss: 38.68331527709961\n",
      "Epoch: 6700 and Loss: 35.76789093017578\n",
      "Epoch: 6800 and Loss: 51.786869049072266\n",
      "Epoch: 6900 and Loss: 41.757484436035156\n",
      "Epoch: 7000 and Loss: 42.137840270996094\n",
      "Epoch: 7100 and Loss: 45.322975158691406\n",
      "Epoch: 7200 and Loss: 37.817405700683594\n",
      "Epoch: 7300 and Loss: 43.387027740478516\n",
      "Epoch: 7400 and Loss: 44.65886306762695\n",
      "Epoch: 7500 and Loss: 38.19137191772461\n",
      "Epoch: 7600 and Loss: 39.15301513671875\n",
      "Epoch: 7700 and Loss: 39.095062255859375\n",
      "Epoch: 7800 and Loss: 37.300209045410156\n",
      "Epoch: 7900 and Loss: 50.350799560546875\n",
      "Epoch: 8000 and Loss: 35.79051208496094\n",
      "Epoch: 8100 and Loss: 36.62379455566406\n",
      "Epoch: 8200 and Loss: 38.033931732177734\n",
      "Epoch: 8300 and Loss: 40.308753967285156\n",
      "Epoch: 8400 and Loss: 37.67008972167969\n",
      "Epoch: 8500 and Loss: 40.36724853515625\n",
      "Epoch: 8600 and Loss: 41.385257720947266\n",
      "Epoch: 8700 and Loss: 42.61275863647461\n",
      "Epoch: 8800 and Loss: 50.981109619140625\n",
      "Epoch: 8900 and Loss: 36.55413818359375\n",
      "Epoch: 9000 and Loss: 40.61635971069336\n",
      "Epoch: 9100 and Loss: 43.37464141845703\n",
      "Epoch: 9200 and Loss: 39.38380813598633\n",
      "Epoch: 9300 and Loss: 43.36861801147461\n",
      "Epoch: 9400 and Loss: 35.844879150390625\n",
      "Epoch: 9500 and Loss: 37.326839447021484\n",
      "Epoch: 9600 and Loss: 40.79816436767578\n",
      "Epoch: 9700 and Loss: 40.569305419921875\n",
      "Epoch: 9800 and Loss: 38.141700744628906\n",
      "Epoch: 9900 and Loss: 39.76722717285156\n",
      "Epoch: 10000 and Loss: 38.22498321533203\n",
      "Epoch: 10100 and Loss: 37.88726043701172\n",
      "Epoch: 10200 and Loss: 38.48704528808594\n",
      "Epoch: 10300 and Loss: 35.421051025390625\n",
      "Epoch: 10400 and Loss: 38.40666961669922\n",
      "Epoch: 10500 and Loss: 35.95766067504883\n",
      "Epoch: 10600 and Loss: 38.14597702026367\n",
      "Epoch: 10700 and Loss: 34.91864776611328\n",
      "Epoch: 10800 and Loss: 38.76030349731445\n",
      "Epoch: 10900 and Loss: 35.15715789794922\n",
      "Epoch: 11000 and Loss: 34.935211181640625\n",
      "Epoch: 11100 and Loss: 37.15026092529297\n",
      "Epoch: 11200 and Loss: 40.23973083496094\n",
      "Epoch: 11300 and Loss: 41.871376037597656\n",
      "Epoch: 11400 and Loss: 37.35578155517578\n",
      "Epoch: 11500 and Loss: 36.474002838134766\n",
      "Epoch: 11600 and Loss: 39.018882751464844\n",
      "Epoch: 11700 and Loss: 42.42018127441406\n",
      "Epoch: 11800 and Loss: 36.35073471069336\n",
      "Epoch: 11900 and Loss: 36.25758361816406\n",
      "Epoch: 12000 and Loss: 37.66831970214844\n",
      "Epoch: 12100 and Loss: 37.59007263183594\n",
      "Epoch: 12200 and Loss: 34.987060546875\n",
      "Epoch: 12300 and Loss: 39.37110137939453\n",
      "Epoch: 12400 and Loss: 43.473331451416016\n",
      "Epoch: 12500 and Loss: 36.67033386230469\n",
      "Epoch: 12600 and Loss: 47.80511474609375\n",
      "Epoch: 12700 and Loss: 39.62689208984375\n",
      "Epoch: 12800 and Loss: 34.6389045715332\n",
      "Epoch: 12900 and Loss: 36.85601043701172\n",
      "Epoch: 13000 and Loss: 37.18718338012695\n",
      "Epoch: 13100 and Loss: 53.46818923950195\n",
      "Epoch: 13200 and Loss: 34.376380920410156\n",
      "Epoch: 13300 and Loss: 37.68125915527344\n",
      "Epoch: 13400 and Loss: 37.95103454589844\n",
      "Epoch: 13500 and Loss: 32.6761360168457\n",
      "Epoch: 13600 and Loss: 33.88018035888672\n",
      "Epoch: 13700 and Loss: 32.19883728027344\n",
      "Epoch: 13800 and Loss: 33.73809051513672\n",
      "Epoch: 13900 and Loss: 36.638397216796875\n",
      "Epoch: 14000 and Loss: 35.76573944091797\n",
      "Epoch: 14100 and Loss: 36.37507247924805\n",
      "Epoch: 14200 and Loss: 36.19130325317383\n",
      "Epoch: 14300 and Loss: 34.91938018798828\n",
      "Epoch: 14400 and Loss: 35.61864471435547\n",
      "Epoch: 14500 and Loss: 34.457054138183594\n",
      "Epoch: 14600 and Loss: 36.42927551269531\n",
      "Epoch: 14700 and Loss: 36.94792938232422\n",
      "Epoch: 14800 and Loss: 38.114219665527344\n",
      "Epoch: 14900 and Loss: 40.13291549682617\n",
      "Epoch: 15000 and Loss: 36.554962158203125\n",
      "Epoch: 15100 and Loss: 37.52684783935547\n",
      "Epoch: 15200 and Loss: 35.080101013183594\n",
      "Epoch: 15300 and Loss: 35.322566986083984\n",
      "Epoch: 15400 and Loss: 35.521583557128906\n",
      "Epoch: 15500 and Loss: 36.99543762207031\n",
      "Epoch: 15600 and Loss: 37.87001037597656\n",
      "Epoch: 15700 and Loss: 37.17452621459961\n",
      "Epoch: 15800 and Loss: 34.508460998535156\n",
      "Epoch: 15900 and Loss: 34.27948760986328\n",
      "Epoch: 16000 and Loss: 38.566123962402344\n",
      "Epoch: 16100 and Loss: 34.17601776123047\n",
      "Epoch: 16200 and Loss: 34.104400634765625\n",
      "Epoch: 16300 and Loss: 41.850589752197266\n",
      "Epoch: 16400 and Loss: 33.47881317138672\n",
      "Epoch: 16500 and Loss: 36.08884048461914\n",
      "Epoch: 16600 and Loss: 33.57982635498047\n",
      "Epoch: 16700 and Loss: 38.238922119140625\n",
      "Epoch: 16800 and Loss: 35.632606506347656\n",
      "Epoch: 16900 and Loss: 37.99372100830078\n",
      "Epoch: 17000 and Loss: 34.69045639038086\n",
      "Epoch: 17100 and Loss: 36.021217346191406\n",
      "Epoch: 17200 and Loss: 34.69856262207031\n",
      "Epoch: 17300 and Loss: 33.41090393066406\n",
      "Epoch: 17400 and Loss: 36.82085418701172\n",
      "Epoch: 17500 and Loss: 33.69013977050781\n",
      "Epoch: 17600 and Loss: 35.83015823364258\n",
      "Epoch: 17700 and Loss: 37.200950622558594\n",
      "Epoch: 17800 and Loss: 34.13573455810547\n",
      "Epoch: 17900 and Loss: 38.926326751708984\n",
      "Epoch: 18000 and Loss: 36.453155517578125\n",
      "Epoch: 18100 and Loss: 36.46986389160156\n",
      "Epoch: 18200 and Loss: 33.95476531982422\n",
      "Epoch: 18300 and Loss: 35.19496154785156\n",
      "Epoch: 18400 and Loss: 35.35736083984375\n",
      "Epoch: 18500 and Loss: 35.98283386230469\n",
      "Epoch: 18600 and Loss: 32.1639404296875\n",
      "Epoch: 18700 and Loss: 32.755653381347656\n",
      "Epoch: 18800 and Loss: 35.64140319824219\n",
      "Epoch: 18900 and Loss: 44.50139617919922\n",
      "Epoch: 19000 and Loss: 36.235233306884766\n",
      "Epoch: 19100 and Loss: 32.911163330078125\n",
      "Epoch: 19200 and Loss: 34.18684387207031\n",
      "Epoch: 19300 and Loss: 33.11014938354492\n",
      "Epoch: 19400 and Loss: 32.15874481201172\n",
      "Epoch: 19500 and Loss: 38.386558532714844\n",
      "Epoch: 19600 and Loss: 35.996585845947266\n",
      "Epoch: 19700 and Loss: 33.58905792236328\n",
      "Epoch: 19800 and Loss: 39.72267150878906\n",
      "Epoch: 19900 and Loss: 38.69879150390625\n",
      "\n",
      "Test accuracy: 0.9747633934020996\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:    \n",
    "    # init all variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    for i in range(total_epochs):\n",
    "        # get mini batch\n",
    "        a, b = get_mini_batch(X_train, y_train)\n",
    "\n",
    "        # run train step, feeding arrays of 100 rows each time\n",
    "        _, cost = sess.run([train_step, loss], feed_dict={x: a, y: b})\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch: {0} and Loss: {1}\".format(i, cost))\n",
    "    \n",
    "    # benchmark neural network performance\n",
    "    result = sess.run(tf_accuracy, feed_dict={x: X_test, y: y_test})\n",
    "    print()\n",
    "    print(\"Test accuracy: {}\".format(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
